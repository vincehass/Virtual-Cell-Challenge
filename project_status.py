#!/usr/bin/env python3
"""
ğŸ“Š Virtual Cell Challenge - Project Status Summary
Shows current progress, available data, and next steps.
"""

import os
from pathlib import Path
import json
from datetime import datetime

def check_project_status():
    """Check the current status of the project."""
    
    print("ğŸ§¬ Virtual Cell Challenge - Project Status")
    print("=" * 50)
    print(f"ğŸ“… Status check: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # Check directories
    print("\nğŸ“ Directory Structure:")
    directories = [
        "data/processed",
        "data/results", 
        "data/vcc_data",
        "scripts",
        "config",
        "notebooks"
    ]
    
    for dir_path in directories:
        path = Path(dir_path)
        if path.exists():
            files = list(path.glob("*"))
            print(f"  âœ… {dir_path} ({len(files)} files)")
        else:
            print(f"  âŒ {dir_path} (missing)")
    
    # Check processed data files
    print("\nğŸ’¾ Available Processed Data:")
    processed_dir = Path("data/processed")
    
    if processed_dir.exists():
        total_size_gb = 0
        recommended_files = []
        large_files = []
        
        for file in processed_dir.glob("*.h5ad"):
            size_gb = file.stat().st_size / (1024**3)
            total_size_gb += size_gb
            
            if size_gb < 2.0:
                recommended_files.append((file.name, size_gb))
                status = "ğŸŸ¢ RECOMMENDED"
            elif size_gb < 10.0:
                large_files.append((file.name, size_gb))
                status = "ğŸŸ¡ LARGE"
            else:
                large_files.append((file.name, size_gb))
                status = "ğŸ”´ VERY LARGE"
            
            print(f"  {status} {file.name} ({size_gb:.1f} GB)")
        
        print(f"\n  ğŸ“Š Total processed data: {total_size_gb:.1f} GB")
        
        if recommended_files:
            print(f"\n  ğŸ¯ BEST for quick analysis:")
            for name, size in recommended_files:
                print(f"    â€¢ {name} ({size:.1f} GB)")
    else:
        print("  âŒ No processed data directory found")
    
    # Check analysis results
    print("\nğŸ“ˆ Previous Analysis Results:")
    results_dir = Path("data/results")
    
    if results_dir.exists():
        json_files = list(results_dir.glob("**/*.json"))
        png_files = list(results_dir.glob("**/*.png"))
        excel_files = list(results_dir.glob("**/*.xlsx"))
        
        print(f"  ğŸ“‹ JSON results: {len(json_files)}")
        print(f"  ğŸ“Š Visualizations: {len(png_files)}")
        print(f"  ğŸ“‘ Excel reports: {len(excel_files)}")
        
        if png_files:
            print("\n  ğŸ–¼ï¸  Available visualizations:")
            for png_file in png_files[-3:]:  # Show last 3
                print(f"    â€¢ {png_file.name}")
    else:
        print("  ğŸ“Š No previous results found")
    
    # Check scripts
    print("\nğŸ”§ Available Analysis Scripts:")
    scripts_dir = Path("scripts")
    
    if scripts_dir.exists():
        python_scripts = list(scripts_dir.glob("*.py"))
        
        key_scripts = {
            "vcc_streamlined_analysis.py": "ğŸ¯ RECOMMENDED - Quick analysis with processed data",
            "vcc_pipeline_memory_optimized_fixed.py": "ğŸ”§ Original memory-optimized pipeline",
            "analyze_real_vcc_data_optimized.py": "ğŸ“Š Optimized data analysis",
            "vcc_complete_pipeline_wandb.py": "ğŸš€ Complete pipeline with W&B"
        }
        
        for script in python_scripts:
            if script.name in key_scripts:
                print(f"  âœ… {script.name} - {key_scripts[script.name]}")
            else:
                print(f"  ğŸ“ {script.name}")
    
    return recommended_files, large_files

def show_next_steps(recommended_files, large_files):
    """Show recommended next steps."""
    
    print("\nğŸ¯ RECOMMENDED NEXT STEPS")
    print("=" * 30)
    
    if recommended_files:
        print("âœ¨ IMMEDIATE ACTION (2-5 minutes):")
        print("  ğŸš€ Run streamlined analysis on small dataset:")
        print("     export WANDB_API_KEY=44be9c406ee4a794b79849b4f13e2748b55d0ae4")
        print("     python scripts/vcc_streamlined_analysis.py")
        print()
        print("  ğŸ“Š This will:")
        print("    â€¢ Auto-select the smallest dataset")
        print("    â€¢ Perform comprehensive analysis")
        print("    â€¢ Create visualizations")
        print("    â€¢ Generate Excel report")
        print("    â€¢ Log to W&B")
        print("    â€¢ Complete in under 5 minutes")
    
    print("\nğŸ”§ DEVELOPMENT WORKFLOW:")
    print("  1. Test on small data (recommended files)")
    print("  2. Refine analysis parameters")
    print("  3. Apply to larger datasets when ready")
    print("  4. Compare results across datasets")
    
    print("\nğŸ“Š ANALYSIS OPTIONS:")
    print("  ğŸ¯ Quick Start: python scripts/vcc_streamlined_analysis.py")
    print("  ğŸ”¬ Full Pipeline: python scripts/vcc_pipeline_memory_optimized_fixed.py")
    print("  ğŸ“ˆ Data Analysis: python scripts/analyze_real_vcc_data_optimized.py")
    
    print("\nğŸ’¡ SMART STRATEGY:")
    print("  â€¢ Use small datasets for rapid iteration")
    print("  â€¢ Perfect your analysis on manageable data")
    print("  â€¢ Scale up to large datasets only when needed")
    print("  â€¢ Avoid disk space issues by working incrementally")
    
    if large_files:
        print(f"\nâš ï¸  LARGE DATASETS AVAILABLE:")
        print("  â€¢ Save these for final analysis")
        print("  â€¢ Use sampling when loading (built into streamlined script)")
        for name, size in large_files[:3]:
            print(f"    â€¢ {name} ({size:.1f} GB)")

def main():
    """Main status check."""
    
    recommended_files, large_files = check_project_status()
    show_next_steps(recommended_files, large_files)
    
    print("\n" + "=" * 50)
    print("ğŸ‰ Your project is ready for streamlined analysis!")
    print("ğŸ’« No external drive needed - work with what you have!")

if __name__ == "__main__":
    main() 