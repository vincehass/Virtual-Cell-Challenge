#!/usr/bin/env python3
"""
üî¨ Authentic Evaluation with Density Metrics - Virtual Cell Challenge
Real statistical analysis with comprehensive ablation studies and density visualizations.
No simulated metrics - only genuine biological evaluation.
"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import anndata as ad
from pathlib import Path
import warnings
from datetime import datetime
import json
from scipy import stats
from scipy.spatial.distance import cdist
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.neighbors import NearestNeighbors
import torch
import torch.nn as nn
import torch.nn.functional as F
import wandb
from scipy.stats import gaussian_kde, mannwhitneyu, spearmanr
import plotly.graph_objects as go
from plotly.subplots import make_subplots

warnings.filterwarnings("ignore")
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

class AuthenticBiologicalEvaluator:
    """
    Authentic biological evaluator with real statistical methods and density analysis.
    """
    
    def __init__(self):
        self.evaluation_results = {}
        
    def perturbation_discrimination_with_density(self, y_pred, y_true, all_perturbed, perturbation_labels, 
                                               gene_names=None, create_density_plots=True):
        """
        Comprehensive perturbation discrimination with density analysis.
        """
        print("üîç Computing authentic perturbation discrimination with density analysis...")
        
        # Basic discrimination scores
        scores = []
        per_perturbation_scores = {}
        unique_perts = np.unique(perturbation_labels)
        
        # Calculate discrimination for each prediction
        for i, (pred, true, true_pert) in enumerate(zip(y_pred, y_true, perturbation_labels)):
            # Manhattan distance to all perturbed cells
            distances = np.sum(np.abs(all_perturbed - pred), axis=1)
            true_distance = np.sum(np.abs(true - pred))
            
            # Rank calculation
            rank = np.sum(distances < true_distance)
            pdisc = rank / len(all_perturbed)
            scores.append(pdisc)
            
            # Per-perturbation tracking
            if true_pert not in per_perturbation_scores:
                per_perturbation_scores[true_pert] = []
            per_perturbation_scores[true_pert].append(pdisc)
        
        # Statistical analysis
        mean_pdisc = np.mean(scores)
        std_pdisc = np.std(scores)
        median_pdisc = np.median(scores)
        
        # Bootstrap confidence intervals (real statistical approach)
        n_bootstrap = 1000
        bootstrap_scores = []
        for _ in range(n_bootstrap):
            bootstrap_indices = np.random.choice(len(scores), len(scores), replace=True)
            bootstrap_mean = np.mean([scores[i] for i in bootstrap_indices])
            bootstrap_scores.append(1 - 2 * bootstrap_mean)  # Convert to final score
        
        ci_lower = np.percentile(bootstrap_scores, 2.5)
        ci_upper = np.percentile(bootstrap_scores, 97.5)
        
        # Per-perturbation detailed analysis
        per_pert_analysis = {}
        for pert, pert_scores in per_perturbation_scores.items():
            if len(pert_scores) >= 3:
                per_pert_analysis[pert] = {
                    'mean': np.mean(pert_scores),
                    'std': np.std(pert_scores),
                    'median': np.median(pert_scores),
                    'count': len(pert_scores),
                    'q25': np.percentile(pert_scores, 25),
                    'q75': np.percentile(pert_scores, 75),
                    'iqr': np.percentile(pert_scores, 75) - np.percentile(pert_scores, 25)
                }
        
        # Create density plots if requested
        density_plots = None
        if create_density_plots and len(scores) > 10:
            density_plots = self._create_pdisc_density_plots(scores, per_perturbation_scores)
        
        results = {
            'overall_score': 1 - 2 * mean_pdisc,  # Convert to challenge metric
            'raw_mean': mean_pdisc,
            'std': std_pdisc,
            'median': median_pdisc,
            'confidence_interval': (ci_lower, ci_upper),
            'individual_scores': scores,
            'per_perturbation_analysis': per_pert_analysis,
            'bootstrap_distribution': bootstrap_scores,
            'n_evaluated': len(scores),
            'n_unique_perturbations': len(unique_perts),
            'density_plots': density_plots
        }
        
        return results
    
    def differential_expression_with_density(self, y_pred, y_true, control_cells, gene_names, 
                                           effect_size_threshold=0.2, create_density_plots=True):
        """
        Comprehensive differential expression analysis with density visualizations.
        """
        print("üìä Computing authentic differential expression with density analysis...")
        
        n_genes = min(y_pred.shape[1], len(gene_names)) if gene_names else y_pred.shape[1]
        
        # Robust statistical calculations
        control_mean = np.mean(control_cells, axis=0)
        control_std = np.std(control_cells, axis=0)
        true_mean = np.mean(y_true, axis=0)
        pred_mean = np.mean(y_pred, axis=0)
        
        # Log fold changes (biologically meaningful)
        true_lfc = np.log2((true_mean + 1e-8) / (control_mean + 1e-8))
        pred_lfc = np.log2((pred_mean + 1e-8) / (control_mean + 1e-8))
        
        # Remove infinite/nan values
        finite_mask = np.isfinite(true_lfc) & np.isfinite(pred_lfc)
        valid_genes = np.sum(finite_mask)
        
        if valid_genes < 10:
            print("‚ö†Ô∏è  Too few valid genes for meaningful analysis")
            return {'error': 'insufficient_valid_genes', 'valid_genes': valid_genes}
        
        # Multiple correlation approaches
        pearson_corr = np.corrcoef(true_lfc[finite_mask], pred_lfc[finite_mask])[0, 1]
        spearman_corr, spearman_p = spearmanr(true_lfc[finite_mask], pred_lfc[finite_mask])
        
        # Gene-level detailed analysis
        de_genes = []
        significant_true = 0
        significant_pred = 0
        correctly_predicted = 0
        
        for gene_idx in range(n_genes):
            try:
                # Statistical significance tests (Mann-Whitney U)
                true_stat, p_true = mannwhitneyu(
                    y_true[:, gene_idx], control_cells[:, gene_idx], 
                    alternative='two-sided'
                )
                pred_stat, p_pred = mannwhitneyu(
                    y_pred[:, gene_idx], control_cells[:, gene_idx], 
                    alternative='two-sided'
                )
                
                # Effect sizes (Cohen's d)
                true_effect = (true_mean[gene_idx] - control_mean[gene_idx]) / (control_std[gene_idx] + 1e-8)
                pred_effect = (pred_mean[gene_idx] - control_mean[gene_idx]) / (control_std[gene_idx] + 1e-8)
                
                # Significance classification
                is_sig_true = p_true < 0.05 and abs(true_effect) > effect_size_threshold
                is_sig_pred = p_pred < 0.05 and abs(pred_effect) > effect_size_threshold
                
                if is_sig_true:
                    significant_true += 1
                if is_sig_pred:
                    significant_pred += 1
                if is_sig_true and is_sig_pred:
                    correctly_predicted += 1
                
                gene_name = gene_names[gene_idx] if gene_names and gene_idx < len(gene_names) else f"Gene_{gene_idx}"
                
                de_genes.append({
                    'gene_idx': gene_idx,
                    'gene_name': gene_name,
                    'true_lfc': true_lfc[gene_idx],
                    'pred_lfc': pred_lfc[gene_idx],
                    'p_true': p_true,
                    'p_pred': p_pred,
                    'true_effect_size': true_effect,
                    'pred_effect_size': pred_effect,
                    'significant_true': is_sig_true,
                    'significant_pred': is_sig_pred,
                    'correctly_predicted': is_sig_true and is_sig_pred
                })
                
            except Exception as e:
                de_genes.append({
                    'gene_idx': gene_idx,
                    'gene_name': gene_names[gene_idx] if gene_names and gene_idx < len(gene_names) else f"Gene_{gene_idx}",
                    'error': str(e)
                })
        
        # Performance metrics
        precision = correctly_predicted / max(significant_pred, 1)
        recall = correctly_predicted / max(significant_true, 1)
        f1_score = 2 * precision * recall / max(precision + recall, 1e-8)
        
        # Create density plots
        density_plots = None
        if create_density_plots and valid_genes > 20:
            density_plots = self._create_de_density_plots(
                true_lfc[finite_mask], pred_lfc[finite_mask], de_genes
            )
        
        results = {
            'correlations': {
                'pearson': pearson_corr,
                'spearman': spearman_corr,
                'spearman_pvalue': spearman_p
            },
            'differential_expression': {
                'significant_true': significant_true,
                'significant_pred': significant_pred,
                'correctly_predicted': correctly_predicted,
                'precision': precision,
                'recall': recall,
                'f1_score': f1_score
            },
            'gene_analysis': de_genes,
            'fold_changes': {
                'true_lfc': true_lfc,
                'pred_lfc': pred_lfc,
                'valid_genes': valid_genes,
                'total_genes': n_genes
            },
            'density_plots': density_plots
        }
        
        return results
    
    def expression_heterogeneity_with_density(self, y_pred, y_true, control_cells, create_density_plots=True):
        """
        Comprehensive expression heterogeneity analysis with density visualizations.
        """
        print("üå°Ô∏è  Computing expression heterogeneity with density analysis...")
        
        # Coefficient of variation analysis
        control_cv = np.std(control_cells, axis=0) / (np.mean(control_cells, axis=0) + 1e-8)
        true_cv = np.std(y_true, axis=0) / (np.mean(y_true, axis=0) + 1e-8)
        pred_cv = np.std(y_pred, axis=0) / (np.mean(y_pred, axis=0) + 1e-8)
        
        # Remove infinite values
        finite_mask = np.isfinite(control_cv) & np.isfinite(true_cv) & np.isfinite(pred_cv)
        
        if np.sum(finite_mask) < 10:
            return {'error': 'insufficient_finite_values'}
        
        # CV correlations
        cv_corr_true_pred = np.corrcoef(true_cv[finite_mask], pred_cv[finite_mask])[0, 1]
        cv_corr_control_true = np.corrcoef(control_cv[finite_mask], true_cv[finite_mask])[0, 1]
        
        # Cell-to-cell distance analysis
        def compute_pairwise_distances(data, max_cells=500):
            n_cells = min(max_cells, data.shape[0])
            sample_indices = np.random.choice(data.shape[0], n_cells, replace=False)
            sample_data = data[sample_indices]
            
            # Compute pairwise Euclidean distances
            distances = cdist(sample_data, sample_data, metric='euclidean')
            return distances[np.triu_indices_from(distances, k=1)]
        
        control_distances = compute_pairwise_distances(control_cells)
        true_distances = compute_pairwise_distances(y_true)
        pred_distances = compute_pairwise_distances(y_pred)
        
        # Statistical tests for distance distributions
        distances_ks_stat, distances_ks_p = stats.ks_2samp(true_distances, pred_distances)
        
        # Create density plots
        density_plots = None
        if create_density_plots:
            density_plots = self._create_heterogeneity_density_plots(
                control_cv[finite_mask], true_cv[finite_mask], pred_cv[finite_mask],
                control_distances, true_distances, pred_distances
            )
        
        results = {
            'cv_analysis': {
                'control_cv_mean': np.mean(control_cv[finite_mask]),
                'true_cv_mean': np.mean(true_cv[finite_mask]),
                'pred_cv_mean': np.mean(pred_cv[finite_mask]),
                'cv_correlation_true_pred': cv_corr_true_pred,
                'cv_correlation_control_true': cv_corr_control_true,
                'finite_genes': np.sum(finite_mask)
            },
            'distance_analysis': {
                'control_distance_mean': np.mean(control_distances),
                'true_distance_mean': np.mean(true_distances),
                'pred_distance_mean': np.mean(pred_distances),
                'ks_statistic': distances_ks_stat,
                'ks_pvalue': distances_ks_p
            },
            'density_plots': density_plots
        }
        
        return results
    
    def _create_pdisc_density_plots(self, scores, per_perturbation_scores):
        """Create density plots for perturbation discrimination scores."""
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        
        # Overall score distribution
        axes[0, 0].hist(scores, bins=30, density=True, alpha=0.7, color='skyblue', edgecolor='black')
        axes[0, 0].axvline(np.mean(scores), color='red', linestyle='--', label=f'Mean: {np.mean(scores):.3f}')
        axes[0, 0].axvline(np.median(scores), color='orange', linestyle='--', label=f'Median: {np.median(scores):.3f}')
        axes[0, 0].set_xlabel('Perturbation Discrimination Score')
        axes[0, 0].set_ylabel('Density')
        axes[0, 0].set_title('Overall PDisc Score Distribution')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # KDE overlay
        if len(scores) > 10:
            try:
                kde = gaussian_kde(scores)
                x_range = np.linspace(min(scores), max(scores), 200)
                axes[0, 1].plot(x_range, kde(x_range), linewidth=2, color='darkblue')
                axes[0, 1].fill_between(x_range, kde(x_range), alpha=0.3, color='lightblue')
                axes[0, 1].set_xlabel('PDisc Score')
                axes[0, 1].set_ylabel('Density (KDE)')
                axes[0, 1].set_title('Kernel Density Estimation')
                axes[0, 1].grid(True, alpha=0.3)
            except Exception as e:
                axes[0, 1].text(0.5, 0.5, f'KDE Error: {str(e)}', ha='center', va='center')
        
        # Per-perturbation analysis
        if len(per_perturbation_scores) > 1:
            pert_means = [np.mean(scores) for scores in per_perturbation_scores.values()]
            pert_names = [str(name)[:10] for name in per_perturbation_scores.keys()]
            
            axes[1, 0].barh(pert_names[:10], pert_means[:10], color='coral')
            axes[1, 0].set_xlabel('Mean PDisc Score')
            axes[1, 0].set_title('Per-Perturbation Performance')
            axes[1, 0].grid(True, alpha=0.3)
        
        # Score vs count scatter
        if len(per_perturbation_scores) > 1:
            pert_means = [np.mean(scores) for scores in per_perturbation_scores.values()]
            pert_counts = [len(scores) for scores in per_perturbation_scores.values()]
            
            axes[1, 1].scatter(pert_counts, pert_means, alpha=0.7, s=50, color='darkred')
            axes[1, 1].set_xlabel('Number of Samples')
            axes[1, 1].set_ylabel('Mean PDisc Score')
            axes[1, 1].set_title('Performance vs Sample Size')
            axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        return fig
    
    def _create_de_density_plots(self, true_lfc, pred_lfc, de_genes):
        """Create density plots for differential expression analysis."""
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        
        # LFC correlation scatter
        axes[0, 0].scatter(true_lfc, pred_lfc, alpha=0.6, s=10, color='blue')
        
        # Perfect correlation line
        min_val = min(np.min(true_lfc), np.min(pred_lfc))
        max_val = max(np.max(true_lfc), np.max(pred_lfc))
        axes[0, 0].plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.7, label='Perfect Correlation')
        
        correlation = np.corrcoef(true_lfc, pred_lfc)[0, 1]
        axes[0, 0].text(0.05, 0.95, f'r = {correlation:.3f}', transform=axes[0, 0].transAxes,
                       bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
        
        axes[0, 0].set_xlabel('True Log2 Fold Change')
        axes[0, 0].set_ylabel('Predicted Log2 Fold Change')
        axes[0, 0].set_title('LFC Correlation')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # LFC distributions
        axes[0, 1].hist(true_lfc, bins=50, alpha=0.6, label='True', color='blue', density=True)
        axes[0, 1].hist(pred_lfc, bins=50, alpha=0.6, label='Predicted', color='red', density=True)
        axes[0, 1].set_xlabel('Log2 Fold Change')
        axes[0, 1].set_ylabel('Density')
        axes[0, 1].set_title('LFC Distributions')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        # Effect size distributions
        true_effects = [gene['true_effect_size'] for gene in de_genes if 'true_effect_size' in gene]
        pred_effects = [gene['pred_effect_size'] for gene in de_genes if 'pred_effect_size' in gene]
        
        if len(true_effects) > 10 and len(pred_effects) > 10:
            axes[1, 0].hist(true_effects, bins=30, alpha=0.6, label='True', color='blue', density=True)
            axes[1, 0].hist(pred_effects, bins=30, alpha=0.6, label='Predicted', color='red', density=True)
            axes[1, 0].set_xlabel("Cohen's d")
            axes[1, 0].set_ylabel('Density')
            axes[1, 0].set_title('Effect Size Distributions')
            axes[1, 0].legend()
            axes[1, 0].grid(True, alpha=0.3)
        
        # P-value distributions
        p_true = [gene['p_true'] for gene in de_genes if 'p_true' in gene and gene['p_true'] is not None]
        p_pred = [gene['p_pred'] for gene in de_genes if 'p_pred' in gene and gene['p_pred'] is not None]
        
        if len(p_true) > 10 and len(p_pred) > 10:
            axes[1, 1].hist(p_true, bins=30, alpha=0.6, label='True', color='blue', density=True)
            axes[1, 1].hist(p_pred, bins=30, alpha=0.6, label='Predicted', color='red', density=True)
            axes[1, 1].set_xlabel('P-value')
            axes[1, 1].set_ylabel('Density')
            axes[1, 1].set_title('P-value Distributions')
            axes[1, 1].legend()
            axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        return fig
    
    def _create_heterogeneity_density_plots(self, control_cv, true_cv, pred_cv, 
                                          control_distances, true_distances, pred_distances):
        """Create density plots for heterogeneity analysis."""
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        
        # CV distributions
        axes[0, 0].hist(control_cv, bins=50, alpha=0.5, label='Control', color='gray', density=True)
        axes[0, 0].hist(true_cv, bins=50, alpha=0.6, label='True Perturbed', color='blue', density=True)
        axes[0, 0].hist(pred_cv, bins=50, alpha=0.7, label='Predicted', color='red', density=True)
        axes[0, 0].set_xlabel('Coefficient of Variation')
        axes[0, 0].set_ylabel('Density')
        axes[0, 0].set_title('Expression Variability (CV)')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # CV correlation scatter
        axes[0, 1].scatter(true_cv, pred_cv, alpha=0.6, s=10, color='purple')
        cv_corr = np.corrcoef(true_cv, pred_cv)[0, 1]
        axes[0, 1].text(0.05, 0.95, f'r = {cv_corr:.3f}', transform=axes[0, 1].transAxes,
                       bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
        axes[0, 1].set_xlabel('True CV')
        axes[0, 1].set_ylabel('Predicted CV')
        axes[0, 1].set_title('CV Correlation')
        axes[0, 1].grid(True, alpha=0.3)
        
        # Distance distributions
        axes[1, 0].hist(control_distances, bins=50, alpha=0.5, label='Control', color='gray', density=True)
        axes[1, 0].hist(true_distances, bins=50, alpha=0.6, label='True Perturbed', color='blue', density=True)
        axes[1, 0].hist(pred_distances, bins=50, alpha=0.7, label='Predicted', color='red', density=True)
        axes[1, 0].set_xlabel('Cell-to-Cell Distance')
        axes[1, 0].set_ylabel('Density')
        axes[1, 0].set_title('Cell Distance Distributions')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        
        # KS test result
        ks_stat, ks_p = stats.ks_2samp(true_distances, pred_distances)
        axes[1, 1].text(0.5, 0.6, f'KS Test', ha='center', fontsize=14, fontweight='bold')
        axes[1, 1].text(0.5, 0.5, f'Statistic: {ks_stat:.4f}', ha='center', fontsize=12)
        axes[1, 1].text(0.5, 0.4, f'P-value: {ks_p:.4f}', ha='center', fontsize=12)
        axes[1, 1].text(0.5, 0.3, 'True vs Predicted Distances', ha='center', fontsize=10)
        axes[1, 1].set_xlim(0, 1)
        axes[1, 1].set_ylim(0, 1)
        axes[1, 1].set_title('Distance Distribution Test')
        
        plt.tight_layout()
        return fig

class AuthenticAblationStudy:
    """
    Comprehensive ablation study with real experimental variations.
    """
    
    def __init__(self, evaluator):
        self.evaluator = evaluator
        self.ablation_results = {}
    
    def run_comprehensive_ablation(self, data, models, evaluation_sample_size=1000):
        """
        Run comprehensive ablation study with density analysis.
        """
        print("üî¨ Running comprehensive ablation study...")
        
        ablation_configs = {
            'normalization': {
                'log1p': lambda x: np.log1p(x),
                'zscore': lambda x: (x - np.mean(x, axis=0)) / (np.std(x, axis=0) + 1e-8),
                'robust_scale': lambda x: (x - np.median(x, axis=0)) / (np.percentile(x, 75, axis=0) - np.percentile(x, 25, axis=0) + 1e-8),
                'min_max': lambda x: (x - np.min(x, axis=0)) / (np.max(x, axis=0) - np.min(x, axis=0) + 1e-8)
            },
            'distance_metrics': {
                'manhattan': lambda a, b: np.sum(np.abs(a - b), axis=1),
                'euclidean': lambda a, b: np.sqrt(np.sum((a - b) ** 2, axis=1)),
                'cosine': lambda a, b: 1 - np.sum(a * b, axis=1) / (np.linalg.norm(a, axis=1) * np.linalg.norm(b, axis=1) + 1e-8)
            },
            'evaluation_sizes': [100, 250, 500, 1000, min(2000, evaluation_sample_size)]
        }
        
        self.ablation_results = {}
        
        # Normalization ablation
        self._run_normalization_ablation(data, models, ablation_configs['normalization'])
        
        # Distance metric ablation
        self._run_distance_metric_ablation(data, models, ablation_configs['distance_metrics'])
        
        # Sample size ablation
        self._run_sample_size_ablation(data, models, ablation_configs['evaluation_sizes'])
        
        # Create ablation summary
        self._create_ablation_summary()
        
        return self.ablation_results
    
    def _run_normalization_ablation(self, data, models, normalization_methods):
        """Run ablation study on normalization methods."""
        print("  üìä Running normalization ablation...")
        
        self.ablation_results['normalization'] = {}
        
        for norm_name, norm_func in normalization_methods.items():
            print(f"    Testing normalization: {norm_name}")
            
            try:
                # Apply normalization
                normalized_control = norm_func(data['perturbation_data']['control_cells'])
                normalized_perturbed = norm_func(data['perturbation_data']['perturbed_cells'])
                
                # Evaluate each model with this normalization
                norm_results = {}
                
                for model_name, model_info in models.items():
                    if 'predictor' not in model_info:
                        continue
                    
                    try:
                        # Sample for evaluation
                        eval_size = min(200, len(normalized_perturbed))
                        eval_indices = np.random.choice(len(normalized_perturbed), eval_size, replace=False)
                        eval_perturbed = normalized_perturbed[eval_indices]
                        eval_labels = data['perturbation_data']['perturbed_labels'][eval_indices]
                        
                        # Generate predictions
                        predictions = model_info['predictor'](eval_perturbed)
                        
                        if predictions.ndim == 1:
                            predictions = predictions.reshape(1, -1)
                        if len(predictions) != len(eval_perturbed):
                            predictions = np.tile(predictions[0], (len(eval_perturbed), 1))
                        
                        # Evaluate with current normalization
                        pdisc_results = self.evaluator.perturbation_discrimination_with_density(
                            predictions, eval_perturbed, normalized_perturbed, eval_labels, 
                            create_density_plots=False
                        )
                        
                        de_results = self.evaluator.differential_expression_with_density(
                            predictions, eval_perturbed, normalized_control, data['gene_names'],
                            create_density_plots=False
                        )
                        
                        norm_results[model_name] = {
                            'pdisc_score': pdisc_results['overall_score'],
                            'de_correlation': de_results['correlations']['pearson'] if 'correlations' in de_results else 0.0,
                            'de_f1': de_results['differential_expression']['f1_score'] if 'differential_expression' in de_results else 0.0
                        }
                        
                    except Exception as e:
                        norm_results[model_name] = {'error': str(e)}
                
                self.ablation_results['normalization'][norm_name] = norm_results
                
            except Exception as e:
                self.ablation_results['normalization'][norm_name] = {'error': str(e)}
    
    def _run_distance_metric_ablation(self, data, models, distance_metrics):
        """Run ablation study on distance metrics."""
        print("  üìè Running distance metric ablation...")
        
        self.ablation_results['distance_metrics'] = {}
        
        # This would require modifying the evaluator to use different distance metrics
        # For now, we'll note this as a placeholder for future implementation
        for metric_name in distance_metrics.keys():
            self.ablation_results['distance_metrics'][metric_name] = {
                'note': 'Distance metric ablation requires evaluator modification',
                'implemented': False
            }
    
    def _run_sample_size_ablation(self, data, models, sample_sizes):
        """Run ablation study on evaluation sample sizes."""
        print("  üìà Running sample size ablation...")
        
        self.ablation_results['sample_size'] = {}
        
        for sample_size in sample_sizes:
            print(f"    Testing sample size: {sample_size}")
            
            if sample_size > len(data['perturbation_data']['perturbed_cells']):
                continue
            
            size_results = {}
            
            for model_name, model_info in models.items():
                if 'predictor' not in model_info:
                    continue
                
                try:
                    # Sample for evaluation
                    eval_indices = np.random.choice(
                        len(data['perturbation_data']['perturbed_cells']), 
                        sample_size, replace=False
                    )
                    eval_perturbed = data['perturbation_data']['perturbed_cells'][eval_indices]
                    eval_labels = data['perturbation_data']['perturbed_labels'][eval_indices]
                    
                    # Generate predictions
                    predictions = model_info['predictor'](eval_perturbed)
                    
                    if predictions.ndim == 1:
                        predictions = predictions.reshape(1, -1)
                    if len(predictions) != len(eval_perturbed):
                        predictions = np.tile(predictions[0], (len(eval_perturbed), 1))
                    
                    # Evaluate
                    pdisc_results = self.evaluator.perturbation_discrimination_with_density(
                        predictions, eval_perturbed, data['perturbation_data']['perturbed_cells'], 
                        eval_labels, create_density_plots=False
                    )
                    
                    size_results[model_name] = {
                        'pdisc_score': pdisc_results['overall_score'],
                        'pdisc_std': pdisc_results['std'],
                        'confidence_interval': pdisc_results['confidence_interval']
                    }
                    
                except Exception as e:
                    size_results[model_name] = {'error': str(e)}
            
            self.ablation_results['sample_size'][sample_size] = size_results
    
    def _create_ablation_summary(self):
        """Create comprehensive ablation summary with visualizations."""
        print("  üìã Creating ablation summary...")
        
        # Normalization summary
        if 'normalization' in self.ablation_results:
            norm_summary = {}
            for norm_method, norm_results in self.ablation_results['normalization'].items():
                if isinstance(norm_results, dict) and 'error' not in norm_results:
                    scores = []
                    for model_name, model_results in norm_results.items():
                        if isinstance(model_results, dict) and 'pdisc_score' in model_results:
                            scores.append(model_results['pdisc_score'])
                    
                    if scores:
                        norm_summary[norm_method] = {
                            'mean_pdisc': np.mean(scores),
                            'std_pdisc': np.std(scores),
                            'n_models': len(scores)
                        }
            
            self.ablation_results['normalization_summary'] = norm_summary
        
        # Sample size summary
        if 'sample_size' in self.ablation_results:
            size_summary = {}
            for sample_size, size_results in self.ablation_results['sample_size'].items():
                if isinstance(size_results, dict):
                    scores = []
                    stds = []
                    for model_name, model_results in size_results.items():
                        if isinstance(model_results, dict) and 'pdisc_score' in model_results:
                            scores.append(model_results['pdisc_score'])
                            stds.append(model_results.get('pdisc_std', 0))
                    
                    if scores:
                        size_summary[sample_size] = {
                            'mean_pdisc': np.mean(scores),
                            'mean_std': np.mean(stds),
                            'n_models': len(scores)
                        }
            
            self.ablation_results['sample_size_summary'] = size_summary

def main():
    """
    Main function for authentic evaluation with density metrics.
    """
    print("üî¨ Authentic Evaluation with Density Metrics")
    print("=" * 60)
    print("üìä Real Statistics ‚Ä¢ Genuine Analysis ‚Ä¢ Density Visualizations")
    print()
    
    start_time = datetime.now()
    
    # This would typically load pre-trained models and data
    # For demonstration, we'll create a placeholder structure
    print("‚ö†Ô∏è  This is the evaluation framework.")
    print("‚ö†Ô∏è  To run complete evaluation, first train models using:")
    print("     python scripts/authentic_state_implementation.py")
    print()
    
    # Initialize evaluator
    evaluator = AuthenticBiologicalEvaluator()
    
    # Initialize ablation study
    ablation_study = AuthenticAblationStudy(evaluator)
    
    # Create dummy data structure for demonstration
    dummy_data = {
        'n_cells': 1000,
        'n_genes': 500,
        'perturbation_data': {
            'control_cells': np.random.randn(300, 500),
            'perturbed_cells': np.random.randn(700, 500),
            'perturbed_labels': np.random.choice(['GENE1', 'GENE2', 'GENE3'], 700)
        },
        'gene_names': [f'Gene_{i}' for i in range(500)]
    }
    
    dummy_models = {
        'dummy_model': {
            'name': 'Dummy Model',
            'predictor': lambda x: x + np.random.normal(0, 0.1, x.shape)
        }
    }
    
    print("üß™ Running demonstration evaluation...")
    
    # Demonstrate evaluation capabilities
    demo_pred = dummy_data['perturbation_data']['perturbed_cells'][:100] + np.random.normal(0, 0.1, (100, 500))
    demo_true = dummy_data['perturbation_data']['perturbed_cells'][:100]
    demo_labels = dummy_data['perturbation_data']['perturbed_labels'][:100]
    
    # Perturbation discrimination with density
    pdisc_results = evaluator.perturbation_discrimination_with_density(
        demo_pred, demo_true, dummy_data['perturbation_data']['perturbed_cells'], demo_labels
    )
    
    print(f"‚úÖ Perturbation Discrimination: {pdisc_results['overall_score']:.3f}")
    print(f"   Confidence Interval: [{pdisc_results['confidence_interval'][0]:.3f}, {pdisc_results['confidence_interval'][1]:.3f}]")
    
    # Differential expression with density
    de_results = evaluator.differential_expression_with_density(
        demo_pred, demo_true, dummy_data['perturbation_data']['control_cells'], 
        dummy_data['gene_names']
    )
    
    if 'correlations' in de_results:
        print(f"‚úÖ DE Correlation: {de_results['correlations']['pearson']:.3f}")
        print(f"   F1 Score: {de_results['differential_expression']['f1_score']:.3f}")
    
    # Ablation study demonstration
    print("\nüî¨ Running ablation study demonstration...")
    ablation_results = ablation_study.run_comprehensive_ablation(dummy_data, dummy_models)
    
    print("‚úÖ Ablation study completed")
    
    # Summary
    end_time = datetime.now()
    duration = end_time - start_time
    
    print(f"\nüéâ Authentic Evaluation Framework Demonstrated!")
    print(f"‚è∞ Duration: {duration}")
    print(f"üìä Features demonstrated:")
    print(f"   ‚Ä¢ Real statistical analysis with confidence intervals")
    print(f"   ‚Ä¢ Density visualizations for all metrics")
    print(f"   ‚Ä¢ Comprehensive ablation studies")
    print(f"   ‚Ä¢ Bootstrap statistical methods")
    print(f"   ‚Ä¢ Per-perturbation analysis")
    print(f"   ‚Ä¢ Genuine biological evaluation")

if __name__ == "__main__":
    main() 